1.python的网页下载器;urllib和requests
scrapy执行原理
1.spider打开网页，获取到一个或者多个request，经由scrpay engine传送给scheduler。request特别多并且速度特别快，会在scheduler形成请求队列queue，由scheduler安排执行
2.scheduler会按照一定的次序取出请求，经由引擎，下载器中间键，发送给下载器downloader
这里的下载器中间键是设定在请求执行前，因此可以设定代理，请求头，cookie等
3.下载下来的网页数据再次经过下载器中间键，经过引擎，经过爬虫中间键传递给爬虫spiders
这里的下载器中
间键是设定在请求执行后，因此可以修改请求的结果
这里的爬虫中间键是设定在数据或者请求到达爬虫之前，与下载器中间键有类似的功能
4.由爬虫spider对下载下来的数据进行解析，按照item设定的数据结构经由爬虫中间键，引擎发送给项目管道itempipline
这里的项目管道itempipline可以对数据进行进一步的清洗，存储操作
这里的爬虫极有可能从数据中解析到进一步的请求request，他会把请求经由引擎重新发送给调度器scheduler，调度器循环执行上述操作
5.项目管道itempipline管理着最后的输出
scrapy项目流程
1.创建项目
进入目标文件夹
在cmd窗口中输入如下命令
scrpy startproject 项目名
2.创建爬虫
cd 项目名
scrpy genspider 爬虫名 起始网址(网址前后加双引号)
另一种方法：scrpy genspider 爬虫名 起始网址(网址前后加双引号) -t crawl
3.编辑爬虫
4.运行爬虫
scrpy crawl 爬虫名
注：该命令必须在项目文件夹中执行
爬虫目录结构：
scrapy.cfg:项目的配置文件
test/:该项目的python模块
test/items.py:项目中的item文件
test/piplines.py:项目中的piplines文件
test/settings.py:项目中的设置文件
test/spiders.py:放置spider代码的目录
2.定义items
item是保存爬取到的数据的容器，使用方法与python字典类似，并且提供了额外保护机制来避免拼写错误导致的未定义字段错误
3.爬虫spider编写
Spider是用户编写用于从单个网站(或者一些网站)爬取数据的类。里面包含了一个用于下载的初始URL,如何跟进网页中的链接以及如何分析页面中的内容，提取生成item的方法
为了创建一个Spider,必须继承scrapy.Spider类，且定义以下三个属性：
name:用于区别spider,该名字必须唯一，spider的名字不能重复
start_urls:包含了spider在启动时进行爬取的url列表。
parse():每个初始url完成下载后生成的Response对象将会作为唯一的参数传递给该函数，该方法负责解析返回的数据（response data）、提取数据（生成item）以及生成需要进一步处理的url的response对象
start_request():必须返回一个迭代的Reques,spider将开始抓取，后续请求将从这些初始请求连续生成
5.response对象的属性和方法
属性：
url:响应地址，包含响应的url的字符串，此属性为只读
status:响应状态码，表示响应的http状态的整数
headers：请求头，包含响应标题的类字典对象，可以使用get()返回具有指定名称的第一个标头值或者geilist()返回具有指定名称的所有标头值来访问值
body:响应体（字节形式），响应的正文
meta:请求对象的meta属性
flags:包含此响应的标志的列表
selector：选择器对象
text 响应文本
request:请求对象
方法:
   copy:返回一个新的响应，是此响应的副本
   css,xpath:为selector的css,xpath方法的映射
   urljoin:用来将响应url与可能的相对url组合构造为绝对地址
   replace([url,status,headers,body,request,flags,cls
]):返回具有相同成员的Response对象。但通过指定的任何关键字参数赋予新值的成员除外。该属性Response.meta是默认复制
selectorlist
方法：
   extract()循环将所有链接地址遍历出来
   extractfirst()第一个链接地址

